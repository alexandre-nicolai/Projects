{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import json \n",
    "import csv \n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the reddit page to scrap\n",
    "\n",
    "def getPushshiftData(before, sub):\n",
    "    # Create URL \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+'&size=1000&before='+str(before)+'&subreddit='+str(sub)\n",
    "\n",
    "    # Request the URL\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Load JSON data from webpage into data variable\n",
    "    data = json.loads(r.text)\n",
    "    \n",
    "    # return the data element which contains all the submissions data\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract required elements from reddit's submissions\n",
    "\n",
    "def collectSubData(subm):\n",
    "    subData = list() \n",
    "    title = subm['title']\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"    \n",
    "    score = subm['score']\n",
    "    created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "    numComms = subm['num_comments']\n",
    "\n",
    "    \n",
    "    subData.append((title,score,created,numComms,flair))\n",
    "    subStats[sub_id] = subData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = \"1609299988\" # Submissions before December 30th using Unix timestamp\n",
    "sub = \"Coronavirus\" # Subreddit to scrape\n",
    "\n",
    "# Count number of submissions scrapped\n",
    "subCount = 0\n",
    "\n",
    "# Dictionnary where data will be stored\n",
    "subStats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getPushshiftData(before, sub)\n",
    "\n",
    "# Scrape all submissions in the subreddit before December 30th\n",
    "\n",
    "while len(data) > 0: \n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "    before = data[+1]['created_utc']\n",
    "    data = getPushshiftData(before, sub)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2.csv\n",
      "380 submissions have been uploaded\n"
     ]
    }
   ],
   "source": [
    "# Create csv and return length\n",
    "\n",
    "def updateSubs_file():\n",
    "    upload_count = 0\n",
    "    filename = input() \n",
    "    file = filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
    "        a = csv.writer(file, delimiter=',')\n",
    "        headers = [\"Title\",\"Score\",\"Publish Date\",\"Total No. of Comments\",\"Flair\"]\n",
    "        a.writerow(headers)\n",
    "        for sub in subStats:\n",
    "            a.writerow(subStats[sub][0])\n",
    "            upload_count+=1\n",
    "            \n",
    "        print(str(upload_count) + \" submissions have been uploaded\")\n",
    "updateSubs_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
